---
title: Using the Local Qwen Model in LobeChat
description: >-
  Easily chat with the locally deployed Qwen model through LobeChat's
  integration with Ollama. Learn how to install and select the Qwen model.
tags:
  - Qwen
  - Qwen Model
  - LobeChat Integration
  - Ollama
  - Local Deployment
---

# Using the Local Qwen Model

<Image alt={'Using Qwen in LobeChat'} cover src={'https://github.com/lobehub/lobe-chat/assets/17870709/b4a01219-e7b1-48a0-888c-f0271b18e3a6'} />

[Qwen](https://github.com/QwenLM/Qwen1.5) is an open-source large language model (LLM) developed by Alibaba Cloud. It is officially described as an evolving AI model that achieves more accurate Chinese language understanding through extensive training data.

<Video src="https://github.com/lobehub/lobe-chat/assets/28616219/31e5f625-8dc4-4a5f-a5fd-d28d0457782d" />

Now, thanks to LobeChat’s integration with [Ollama](https://ollama.com/), you can easily use the Qwen model locally within LobeChat.

This guide will walk you through how to use the locally deployed Qwen model in LobeChat:

<Steps>
  ### Install Ollama Locally

  First, you’ll need to install Ollama. For installation instructions, refer to the [Ollama usage guide](/en/docs/usage/providers/ollama).

  ### Pull the Qwen Model Using Ollama

  Once Ollama is installed, you can pull the Qwen model locally. For example, to pull the 14b version of the model, run:

  ```bash
  ollama pull qwen:14b
  ```

  <Image alt={'Pulling the Qwen model using Ollama'} height={473} inStep src={'https://github.com/lobehub/lobe-chat/assets/1845053/fe34fdfe-c2e4-4d6a-84d7-4ebc61b2516a'} />

  ### Select the Qwen Model

  In the chat interface, open the model selection panel and choose the Qwen model.

  <Image alt={'Selecting the Qwen model in the model panel'} height={430} inStep src={'https://github.com/lobehub/lobe-chat/assets/28616219/e0608cca-f62f-414a-bc55-28a61ba21f14'} />

  <Callout type={'info'}>
    If you don’t see the Ollama provider in the model selection panel, refer to the [Ollama Integration Guide](/en/docs/self-hosting/examples/ollama) to learn how to enable the Ollama provider in LobeChat.
  </Callout>
</Steps>

You’re now ready to start chatting with the local Qwen model in LobeChat.
