---
title: Using the Google Gemma Model in LobeChat
description: >-
  Easily perform natural language processing tasks with the Google Gemma model
  through LobeChat's integration with Ollama. Install Ollama, pull the Gemma
  model, select it from the model panel, and start chatting.
tags:
  - Google Gemma
  - LobeChat
  - Ollama
  - Natural Language Processing
  - Model Selection
---

# Using the Google Gemma Model

<Image alt={'Using Gemma in LobeChat'} cover rounded src={'https://github.com/lobehub/lobe-chat/assets/17870709/65d2dd2a-fdcf-4f3f-a6af-4ed5164a510d'} />

[Gemma](https://blog.google/technology/developers/gemma-open-models/) is an open-source large language model (LLM) developed by Google. It is designed to be a general-purpose and flexible model for a wide range of natural language processing (NLP) tasks. Now, thanks to LobeChat’s integration with [Ollama](https://ollama.com/), you can easily use Google Gemma directly within LobeChat.

This guide will walk you through how to use the Google Gemma model in LobeChat:

<Steps>
  ### Install Ollama Locally

  First, you’ll need to install Ollama. For installation instructions, refer to the [Ollama usage guide](/en/docs/usage/providers/ollama).

  ### Pull the Google Gemma Model Using Ollama

  Once Ollama is installed, you can pull the Google Gemma model locally. For example, to pull the 7b model, run the following command:

  ```bash
  ollama pull gemma
  ```

  <Image alt={'Pulling the Gemma model using Ollama'} height={473} inStep src={'https://github.com/lobehub/lobe-chat/assets/28616219/7049a811-a08b-45d3-8491-970f579c2ebd'} width={791} />

  ### Select the Gemma Model

  In the chat interface, open the model selection panel and choose the Gemma model.

  <Image alt={'Selecting the Gemma model in the model panel'} height={629} inStep src={'https://github.com/lobehub/lobe-chat/assets/28616219/69414c79-642e-4323-9641-bfa43a74fcc8'} width={791} />

  <Callout type={'info'}>
    If you don’t see the Ollama provider in the model selection panel, refer to the [Ollama Integration Guide](/en/docs/self-hosting/examples/ollama) to learn how to enable the Ollama provider in LobeChat.
  </Callout>
</Steps>

You’re all set! You can now start chatting with the local Gemma model directly in LobeChat.
