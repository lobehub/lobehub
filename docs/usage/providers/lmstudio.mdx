---
title: Using LM Studio in LobeChat
description: >-
  Learn how to configure and use LM Studio to run AI models for conversations
  within LobeChat.
tags:
  - LobeChat
  - LM Studio
  - Open Source Models
  - Web UI
---

# Using LM Studio in LobeChat

<Image alt={'Using LM Studio in LobeChat'} cover src={'https://github.com/user-attachments/assets/cc1f6146-8063-4a4d-947a-7fd6b9133c0c'} />

[LM Studio](https://lmstudio.ai/) is a platform designed for testing and running large language models (LLMs). It offers an intuitive and user-friendly interface, making it ideal for developers and AI enthusiasts. LM Studio supports deploying and running various open-source LLMs locally—such as Deepseek or Qwen—enabling offline AI chatbot functionality that enhances privacy and flexibility.

This guide will walk you through how to use LM Studio within LobeChat:

<Steps>
  ### Step 1: Download and Install LM Studio

  - Visit the [official LM Studio website](https://lmstudio.ai/)
  - Choose your operating system and download the installer. LM Studio currently supports macOS, Windows, and Linux
  - Follow the installation instructions and launch LM Studio

  <Image alt={'Install and launch LM Studio'} inStep src={'https://github.com/user-attachments/assets/e887fa04-c553-45f1-917f-5c123ac9c68b'} />

  ### Step 2: Search and Download a Model

  - Open the `Discover` tab on the left sidebar to search for models
  - Find a model you’d like to use (e.g., Deepseek R1) and click to download
  - The download may take some time—please be patient

  <Image alt={'Search and download a model'} inStep src={'https://github.com/user-attachments/assets/f878355f-710b-452e-8606-0c75c47f29d2'} />

  ### Step 3: Deploy and Run the Model

  - Use the model selector at the top to choose the downloaded model and load it
  - In the pop-up panel, configure the model’s runtime parameters. For detailed settings, refer to the [LM Studio documentation](https://lmstudio.ai/docs)

  <Image alt={'Configure model runtime parameters'} inStep src={'https://github.com/user-attachments/assets/dba58ea6-7df8-4971-b6d4-b24d5f486ba7'} />

  - Click the `Load Model` button and wait for the model to fully load and start
  - Once loaded, you can begin chatting with the model in the built-in interface

  ### Step 4: Enable Local API Service

  - To use the model with other applications, you’ll need to start a local API service. This can be done via the `Developer` panel or from the app menu. By default, LM Studio runs the service on port `1234`

  <Image alt={'Start local API service'} inStep src={'https://github.com/user-attachments/assets/08ced88b-4968-46e8-b1da-0c04ddf5b743'} />

  - After starting the service, make sure to enable the `CORS (Cross-Origin Resource Sharing)` option in the service settings. This is required for external applications to access the model

  <Image alt={'Enable CORS'} inStep src={'https://github.com/user-attachments/assets/8ce79bd6-f1a3-48bb-b3d0-5271c84801c2'} />

  ### Step 5: Connect LM Studio to LobeChat

  - Go to the `App Settings` in LobeChat and open the `AI Service Providers` section
  - Find and select the `LM Studio` provider from the list

  <Image alt={'Enter LM Studio API address'} inStep src={'https://github.com/user-attachments/assets/143ff392-97b5-427a-97a7-f2f577915728'} />

  - Enable the LM Studio provider and enter the API service address

  <Callout type={'warning'}>
    If LM Studio is running locally, make sure to enable the "Client Request Mode".
  </Callout>

  - Add the model you’re running to the model list below
  - Choose a model for your assistant and start chatting

  <Image alt={'Select LM Studio model'} inStep src={'https://github.com/user-attachments/assets/bd399cef-283c-4706-bdc8-de9de662de41'} />
</Steps>

And that’s it! You’re now ready to use models running in LM Studio directly within LobeChat.
